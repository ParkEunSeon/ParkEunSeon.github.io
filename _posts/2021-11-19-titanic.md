---
layout: post
title:  "타이타닉 생존율 예측"
date:   2021-11-19 18:00:00 +0700
categories: [merchinelearning]
---





1.  데이터로드

   ```python
   import pandas as pd
   
   # 데이터로드
   titanic_df = pd.read_csv('titanic_1.csv')
   titanic_df
   ```

   > \* 각 컬럼별 데이터 의미
   >
   >  \- Passengerid : 탑승자 데이터 일련번호
   >
   >  \- survived : 생존 여부, 0=사망, 1=생존
   >
   >  \- Pclass : 티켓의 선실 등급, 1=일등석, 2=이등석, 3=삼등석
   >
   >  \- sex : 탑승자 성별
   >
   >  \- name : 탑승자 이름
   >
   >  \- Age : 탑승자 나이
   >
   >  \- sibsp : 같이 탑승한 형제자매 또는 배우자 인원수
   >
   >  \- parch : 같이 탑승한 부모님 또는 어린이 인원수
   >
   >  \- ticket : 티켓 번호
   >
   >  \- fare : 요금
   >
   >  \- cabin : 선실 번호
   >
   >  \- embarked : 중간 정착 항구 C = Cherbourg, Q = Queenstown, S = Southampton

2. 결측치 처리

   - info를 사용해 컬럼별 널값여부, 데이터 타입등을 확인

     ```python
     titanic_df.info()
     ```

     ![image-20220108222831178](C:\Users\espark\AppData\Roaming\Typora\typora-user-images\image-20220108222831178.png)

     > Age, Cabin, Fare(객실요금) null 값 존재하므로 결측치 처리
     >
     > Fare는 객실요금이므로 생존율에는 영향을 미치지 않을 것으로 판단해 결측치를 처리하지 않음
     >
     > object로 되어있는 컬럼을 인코딩 해야함

```python
titanic_df['Age'].mean()
# 나이는 전체 평균값으로 채운다.
# inplace = False로 하면 원본 데이터 프레임을 변경하지 않고 
# 새로운 데이터프레임을 반환함
titanic_df['Age'].fillna(titanic_df['Age'].mean(), inplace=True)
titanic_df['Fare'].fillna(titanic_df['Fare'].min(), inplace=True)
titanic_df['Cabin'].fillna('N', inplace=True)
```

- ​		컬럼별 데이터 분포 확인	

```python
# 성별 승객 수
print(f"남여 인원수는 : {titanic_df['Sex'].value_counts()}")
# 요금별 승객 수
print(f"요금분포수는 : {titanic_df['Fare'].value_counts()}")
# 탑승지역별 승객 수
print(f"탑승지역별 수는 :{titanic_df['Embarked'].value_counts()}")
# 선실번호별 승객 수
print(f"선실번호별 수는 :{titanic_df['Cabin'].value_counts()}")
```

-  성별 생존자 수
  - DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True)

```python
# 성별 생존자 수(1:생존, 0:사망)
titanic_df.groupby(['Sex','Survived'])['Survived'].count()
```

3. 데이터 전처리

   - 레이블 인코딩

     ```python
     from sklearn.preprocessing import LabelEncoder
     # 데이터프레임이 인자로 들어가면, 특정 컬럼들을 레이블 인코딩해주는 함수
     def encode_fetures(dataFrame):
         features = ['Cabin', 'Sex', 'Embarked']
         for feature in features:
             le = LabelEncoder()
             le.fit(dataFrame[feature])
             dataFrame[feature] = le.transform(dataFrame[feature]) 
         return dataFrame
     titanic_df = encode_fetures(titanic_df)
     titanic_df.head()
     ```

   - 결측치 제거

   ```python
   from sklearn.preprocessing import LabelEncoder
   
   def fillna(df):
       df['Age'].fillna(df['Age'].mean(), inplace=True)
       df['Cabin'].fillna('N', inplace=True)
       df['Embarked'].fillna('N', inplace=True)
       df['Fare'].fillna(0, inplace=True)
       return df
   
   titanic_df['Cabin'].unique()
   ```

   - 불필요한 속성 제거

   ```python
   # 머신러닝 알고리즘에 불필요한 속성 제거
   def drop_features(df):
       df.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)
       df.drop(['WikiId', 'Name_wiki','Age_wiki','Hometown', 'Boarded', 'Destination', 'Lifeboat', 'Body',
          'Class'],axis=1,inplace=True)
       return df
   
   ```

   - 레이블 인코딩 수행

     ```python
     def format_features(df):
         df['Cabin'] = df['Cabin'].str[:1]
         features = ['Cabin','Sex','Embarked']
         for feature in features:
             le = LabelEncoder()
             le = le.fit(df[feature])
             df[feature] = le.transform(df[feature])
         return df
     ```

     ```python
     # 앞에서 설정한 Data Preprocessing 함수 호출
     def transform_features(df):
         df = fillna(df)
         df = drop_features(df)
         df = format_features(df)
         return df
     ```

   - 데이터프레임 분리

     ```python
     # X, y 데이터프레임 분리
     y_titanic_df = titanic_df['Survived']
     X_titanic_df = titanic_df.drop('Survived', axis=1)
     
     # X_train 데이터에 전처리 수행
     X_titanic_df = transform_features(X_titanic_df)
     ```

     ```python
     X_titanic_df['Pclass'].isnull().count()
     X_titanic_df
     ```

     ```python
     from sklearn.preprocessing import MinMaxScaler
     # MinMaxScaler()를 이용해 정규화하기
     def normalize_features(df):
         ms = MinMaxScaler()
         ms.fit(df)
         df = ms.transform(df)
         
         return df
     
     X_normalize_titanic_df = normalize_features(X_titanic_df)
     ```

     ```python
     X_titanic_df_normalize = pd.DataFrame(X_normalize_titanic_df, columns=['survived','Pclass', 'Sex','Age', 'SibSp', 'Parch', 'Fare', 'Cabin' 'Embarked'])
     ```

   - trainSet과 testSet으로 분리

     ```python
     from sklearn.model_selection import train_test_split
     
     X_train, X_test, y_train, y_test = train_test_split(X_titanic_df_normalize, y_titanic_df, test_size = 0.2, random_state=11)
     ```

4. 머신러닝

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 학습
def model_predict(clf,X_train, X_test, y_train, y_test):
    clf.fit(X_train,y_train)
    pred = clf.predict(X_test)
    return pred

# 정확도
def model_accuracy(y_test, pred):
    score = accuracy_score(y_test, pred)
    return score

de_clf = DecisionTreeClassifier(random_state=10)
rf_clf = RandomForestClassifier(random_state=10)
lr_clf = LogisticRegression(random_state=10)

clf_list = [de_clf,rf_clf,lr_clf]
score_list = [] 

for i in range(len(clf_list)):
    pred = model_predict(clf_list[i], X_train, X_test, y_train, y_test)
    score = model_accuracy(y_test, pred)
    score_list.append(score)
    
print(f'DecisionTreeClassifier의 정확도 : {score_list[0]}')
print(f'RandomForestClassifier의 정확도 : {score_list[1]}')
print(f'LogisticRegression의 정확도 : {score_list[2]}')
```

```python
import warnings

from sklearn.model_selection import cross_val_score 


warnings.filterwarnings('ignore')
clf_list = [de_clf,rf_clf,lr_clf]

cross_val_list = []
for i in range(3):
    score = cross_val_score(clf_list[i], X_train, y_train, cv=5)
    score = score.mean()
    cross_val_list.append(score)
    
cross_val_list
```



#### 1. 모델링 절차

![image-20220109004637520](C:\Users\espark\AppData\Roaming\Typora\typora-user-images\image-20220109004637520.png)

#### 2. DecisionTreeClassifier

- 결정트리 머신러닝의 특징
  - 분할과 가지치기 과정을 반복하면서 모델을 생성함
  - 분류와 회귀 알고리즘에 사용함
  - 여러개의 모델을 함께 사용하는 앙상블 모델 존재(RandomForest,  GradientBoosting, XGBoost)
  - 각 특성이 개별처리되기 때문에 데이터 스케일에 영향을 받지 않음
  - 시계열 데이터와 같이 범위 밖의 포인트는 예측할 수 없음
  - 과대적합되는 경향이 존재함
- 사용방법
  - DecisionTreeClassifier(criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, random_state, max_leaf_nodes,  min_impurity_decrease, min_impurity_split, class_weight, presort)
    - criterion : 분할 품질을 측정하는 기능 (default : gini)
    - splitter : 각 노드에서 분할을 선택하는 데 사용되는 전략 (default : best)
    - max_depth : 트리의 최대 깊이 (값이 클수록 모델의 복잡도가 올라간다.)
    - min_samples_split : 자식 노드를 분할하는데 필요한 최소 샘플 수 (default : 2)
    - min_samples_leaf : 리프 노드에 있어야 할 최소 샘플 수 (default : 1)
    - min_weight_fraction_leaf : min_sample_leaf와 같지만 가중치가 부여된 샘플 수에서의 비율
    - max_features : 각 노드에서 분할에 사용할 특징의 최대 수
    - random_state : 난수 seed 설정
    - max_leaf_nodes : 리프 노드의 최대수
    - min_impurity_decrease : 최소 불순도
    - min_impurity_split : 나무 성장을 멈추기 위한 임계치
    - class_weight : 클래스 가중치
    - presort : 데이터 정렬 필요 여부

#### 3. RandomForestClassifier

- 랜덤포레스트 특징
  - 기본 결정트리는 과적합이 많이 발생하여 이를 개선하기 위해 2001년 앙상블 기법으로 고안됨
  - 훈련 과정에서 구성한 다수의 결정트리들을 랜덤하게 학습시켜 분류 또는 회귀 알고리즘에 이용함
  - 각각의 트리가 독립적으로 학습하기 때문에 학습과정을 병렬화할 수 있음
  - 랜덤하게 만들어지기 때문에 random_state를 고정해야 같은 결과를 만들어낼 수 있음
- 사용방법
  - **RandomForestClassifier**(n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, min_impurity_decrease, min_impurity_split, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start, class_weight)
    - n_estimators : 모델에서 사용할 트리 갯수(학습시 생성할 트리 갯수)
    - criterion : 분할 품질을 측정하는 기능 (default : gini)
    - max_depth : 트리의 최대 깊이
    - min_samples_split : 내부 노드를 분할하는데 필요한 최소 샘플 수 (default : 2)
    - min_samples_leaf : 리프 노드에 있어야 할 최소 샘플 수 (default : 1)
    - min_weight_fraction_leaf : min_sample_leaf와 같지만 가중치가 부여된 샘플 수에서의 비율
    - max_features : 각 노드에서 분할에 사용할 특징의 최대 수
    - max_leaf_nodes : 리프 노드의 최대수
    - min_impurity_decrease : 최소 불순도
    - min_impurity_split : 나무 성장을 멈추기 위한 임계치
    - bootstrap : 부트스트랩(중복허용 샘플링) 사용 여부
    - oob_score : 일반화 정확도를 줄이기 위해 밖의 샘플 사용 여부
    - n_jobs :적합성과 예측성을 위해 병렬로 실행할 작업 수
    - random_state : 난수 seed 설정
    - verbose : 실행 과정 출력 여부
    - warm_start : 이전 호출의 솔루션을 재사용하여 합계에 더 많은 견적가를 추가
    - class_weight : 클래스 가중치

#### 4. LogisticRegression

- LogisticRegression 특징

  - 이전의 선형분석과 가장 큰 차이점은 '값'이 아닌 '확률'로서 분류한다는 점임
  - 기존의 선형회귀분석에서는 단순히 입력한 값을 그대로 **독립변수**로 받아서 사용하다 보니 평균보다 차이가 큰 값이 입력되면 값이 크게 달라져버리는 문제점이 존재함
  - 시그모이드라는 계산을 사용함(입력값을 비율로 변환하여 항상 0 과 1 사이 값으로 한정시킴)

- 시그모이드 계산

  - 입력값이 어느 숫자든 상관없이 결괏값이 항상 0과 1사이에 있도록 하며 중간값인 **0.5 이상의 값은 성공, 그 이하의 값은 실패**로 계산함

    ![img](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)

  -  x축의 -6부터 6까지의 값은 입력값인 독립변수를 의미하고, y축의 0부터 1까지 값은 종속변수를 의미함

  -  즉 -6일 때는 결괏값이 0, 6일 때는 결괏값이 1이라는 뜻이고 사이의 값은 0 부터 1 사이 값으로 나오며 결괏값 0.5(중앙값)를 기준으로 왼쪽은 실패, 오른쪽은 성공으로 계산함

  - 시그모이드 함수 사용법

    - import Library

      ```python
      import numpy as np
      import matplotlib.pyplot as plt
      ```

    - 함수 생성

      ```python
      def sigmoid(x):
          a = []
          for itr in x:
              a.append(1 / (1 + np.exp(-itr)))
          return a
      ```

    - Input Data

      ```python
      x = np.linspce(-8, 8, 100)
      sig = sigmoid(x)
      ```

    - 그래프그리기

      ```python
      plt.plot(x, sig)
      plt.grid(True)
      plt.show()
      ```

      

- 사용방법
  - **LogisticRegression**(penalty, dual, tol, C, fit_intercept, intercept_scaling, class_weight, random_state,  solver, max_iter, multi_class, verbose, warm_start, n_jobs, l1_ratio)
    - penalty : 규제에 사용 된 기준을 지정 (l1, l2, elasticnet, none) – default : l2
    - dual : 이중 또는 초기 공식
    - tol : 정밀도
    - C : 규제 강도
    - fit_intercept : 모형에 상수항 (절편)이 있는가 없는가를 결정하는 인수 (default : True)
    - intercept_scaling : 정규화 효과 정도
    - class_weight : 클래스의 가중치
    - random_state : 난수 seed 설정
    - solver : 최적화 문제에 사용하는 알고리즘
    - max_iter : 계산에 사용할 작업 수
    - multi_class : 다중 분류 시에 (ovr, multinomial, auto)로 설정
    - verbose : 동작 과정에 대한 출력 메시지
    - warm_start : 이전 모델을 초기화로 적합하게 사용할 것인지 여부
    - n_jobs : 병렬 처리 할 때 사용되는 CPU 코어 수
    - l1_ratio : L1 규제의 비율(Elastic-Net 믹싱 파라미터 경우에만 사용)

